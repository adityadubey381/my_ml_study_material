{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95ab9d6b-0d2f-4437-88a7-a8cd14215c5a",
   "metadata": {},
   "source": [
    " # CatBoost\r\n",
    "\r\n",
    "### What is CatBoost?\r\n",
    "\r\n",
    "CatBoost is an **advanced gradient boosting** algorithm developed by **Yandex**. It's designed to handle categorical data effectively and solve the common issues associated with other boosting methods. The name **\"CatBoost\"** comes from **Categorical** data and **Boosting**.\r\n",
    "\r\n",
    "But why is it special?\r\n",
    "\r\n",
    "Most gradient boosting algorithms, like **XGBoost** or **LightGBM**, require you to preprocess categorical features into numerical ones using techniques like **one-hot encoding** or **label encoding**. But with **CatBoost**, you don’t need to do that! It handles categorical data directly in a **smart, efficient** way using **ordered boosting**, which leads to better performance.\r\n",
    "\r\n",
    "### How does CatBoost work?\r\n",
    "\r\n",
    "CatBoost works like most boosting algorithms:\r\n",
    "1. **Boosting** means we build trees sequentially. Each new tree tries to fix the mistakes (residuals) of the previous trees.\r\n",
    "2. It’s a **decision tree-based algorithm**. However, what sets CatBoost apart is the way it handles categorical features. It uses **ordered boosting**, a technique that reduces overfitting, which can be an issue with standard gradient boosting.\r\n",
    "\r\n",
    "**Ordered Boosting** helps by considering the categorical features not just as a group but by using **previous trees** as a context, which prevents data leakage and makes the model **more generalizable**.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### CatBoost vs. Other Models (XGBoost, LightGBM, AdaBoost, etc.)\r\n",
    "\r\n",
    "Now, let's compare **CatBoost** with other popular models like **AdaBoost**, **XGBoost**, and **LightGBM**.\r\n",
    "\r\n",
    "#### 1. **AdaBoost**:\r\n",
    "- **What it is**: AdaBoost is a boosting technique that combines multiple weak classifiers (like decision trees) to form a strong classifier. It focuses on misclassified data and adjusts the weights of the samples accordingly.\r\n",
    "- **How it differs from CatBoost**: \r\n",
    "  - AdaBoost works with simple models (weak learners), while CatBoost focuses on building **decision trees**.\r\n",
    "  - **AdaBoost** is more prone to **overfitting** when dealing with noisy data, while **CatBoost** handles overfitting better with its **ordered boosting** technique and is less sensitive to outliers.\r\n",
    "\r\n",
    "#### 2. **XGBoost**:\r\n",
    "- **What it is**: XGBoost stands for **Extreme Gradient Boosting**. It’s one of the most widely used algorithms in Kaggle competitions due to its **speed** and **performance**.\r\n",
    "- **How it differs from CatBoost**: \r\n",
    "  - XGBoost handles numerical features well but requires you to **preprocess categorical features** (e.g., using one-hot encoding or label encoding).\r\n",
    "  - **CatBoost**, on the other hand, **automatically handles categorical features**, so you don't need to spend time on preprocessing.\r\n",
    "  - **CatBoost** also uses **ordered boosting**, which can give it an edge in terms of **accuracy** and **generalization** over XGBoost, especially with categorical data.\r\n",
    "\r\n",
    "#### 3. **LightGBM**:\r\n",
    "- **What it is**: LightGBM is another **gradient boosting algorithm** that focuses on speed and efficiency, especially with large datasets.\r\n",
    "- **How it differs from CatBoost**:\r\n",
    "  - Both **LightGBM** and **CatBoost** are optimized for **speed** and **handling large datasets**.\r\n",
    "  - **LightGBM** also handles categorical features but in a different way. It uses a **leaf-wise growth strategy** which can sometimes lead to overfitting, while **CatBoost** uses **ordered boosting** to mitigate this risk.\r\n",
    "  - **CatBoost** generally performs better with **categorical features** compared to LightGBM, as LightGBM sometimes requires complex preprocessing for them.\r\n",
    "\r\n",
    "#### 4. **HCBoost**:\r\n",
    "- **What it is**: HCBoost (or **Histogram-based CatBoost**) is a variation of CatBoost that focuses on using histograms to speed up training. It's optimized for large-scale datasets.\r\n",
    "- **How it differs from CatBoost**: \r\n",
    "  - **HCBoost** is essentially a faster and more memory-efficient version of CatBoost. It leverages histograms to approximate the gradients, leading to better performance when you're dealing with massive datasets.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### Which One is Best? \r\n",
    "\r\n",
    "- **For Small to Medium Datasets**: **CatBoost** usually gives the best performance when working with categorical features without needing a lot of preprocessing. It also has **less overfitting** and can handle noise well.\r\n",
    "  \r\n",
    "- **For Speed**: If you're focusing on the **speed** of training and working with large datasets, **LightGBM** could be your best bet due to its leaf-wise growth strategy.\r\n",
    "\r\n",
    "- **For Accuracy and Generalization**: If you're working on datasets that have a lot of **categorical features**, **CatBoost** is usually the top choice. It handles these features better than both **XGBoost** and **LightGBM**.\r\n",
    "\r\n",
    "- **For Handling Large Datasets**: **HCBoost** can offer **better performance** on large datasets, especially if you have memory constraints.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### Conclusion\r\n",
    "\r\n",
    "In summary:\r\n",
    "- **CatBoost** shines when you have **categorical data**, as it doesn’t require heavy preprocessing, and it handles it efficiently with **ordered boosting**.\r\n",
    "- **XGBoost** is a great choice for performance and is well-established, but it needs **preprocessing**.\r\n",
    "- **LightGBM** is fast and efficient but can sometimes overfit.\r\n",
    "- **AdaBoost** works with weak learners and may not be as strong as CatBoost for complex datasets.\r\n",
    "\r\n",
    "So, the **best model** truly depends on your sphe concept clearer! Let me know if you need any more details or examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e39be85-84c0-4def-9654-31027814e033",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "053befa2-796c-42d0-add5-11c5b7b25a31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting catboost\n",
      "  Downloading catboost-1.2.7-cp312-cp312-win_amd64.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: graphviz in c:\\users\\aditya kumar dubey\\anaconda3\\envs\\dubey\\lib\\site-packages (from catboost) (0.20.3)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\aditya kumar dubey\\anaconda3\\envs\\dubey\\lib\\site-packages (from catboost) (3.10.0)\n",
      "Collecting numpy<2.0,>=1.16.0 (from catboost)\n",
      "  Using cached numpy-1.26.4-cp312-cp312-win_amd64.whl.metadata (61 kB)\n",
      "Requirement already satisfied: pandas>=0.24 in c:\\users\\aditya kumar dubey\\anaconda3\\envs\\dubey\\lib\\site-packages (from catboost) (2.2.3)\n",
      "Requirement already satisfied: scipy in c:\\users\\aditya kumar dubey\\anaconda3\\envs\\dubey\\lib\\site-packages (from catboost) (1.15.1)\n",
      "Requirement already satisfied: plotly in c:\\users\\aditya kumar dubey\\anaconda3\\envs\\dubey\\lib\\site-packages (from catboost) (6.0.0)\n",
      "Requirement already satisfied: six in c:\\users\\aditya kumar dubey\\anaconda3\\envs\\dubey\\lib\\site-packages (from catboost) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\aditya kumar dubey\\anaconda3\\envs\\dubey\\lib\\site-packages (from pandas>=0.24->catboost) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\aditya kumar dubey\\anaconda3\\envs\\dubey\\lib\\site-packages (from pandas>=0.24->catboost) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\aditya kumar dubey\\anaconda3\\envs\\dubey\\lib\\site-packages (from pandas>=0.24->catboost) (2025.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\aditya kumar dubey\\anaconda3\\envs\\dubey\\lib\\site-packages (from matplotlib->catboost) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\aditya kumar dubey\\anaconda3\\envs\\dubey\\lib\\site-packages (from matplotlib->catboost) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\aditya kumar dubey\\anaconda3\\envs\\dubey\\lib\\site-packages (from matplotlib->catboost) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\aditya kumar dubey\\anaconda3\\envs\\dubey\\lib\\site-packages (from matplotlib->catboost) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\aditya kumar dubey\\anaconda3\\envs\\dubey\\lib\\site-packages (from matplotlib->catboost) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\aditya kumar dubey\\anaconda3\\envs\\dubey\\lib\\site-packages (from matplotlib->catboost) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\aditya kumar dubey\\anaconda3\\envs\\dubey\\lib\\site-packages (from matplotlib->catboost) (3.2.1)\n",
      "Requirement already satisfied: narwhals>=1.15.1 in c:\\users\\aditya kumar dubey\\anaconda3\\envs\\dubey\\lib\\site-packages (from plotly->catboost) (1.30.0)\n",
      "Downloading catboost-1.2.7-cp312-cp312-win_amd64.whl (101.7 MB)\n",
      "   ---------------------------------------- 0.0/101.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.8/101.7 MB 6.7 MB/s eta 0:00:16\n",
      "    --------------------------------------- 2.1/101.7 MB 5.9 MB/s eta 0:00:17\n",
      "   - -------------------------------------- 3.4/101.7 MB 6.3 MB/s eta 0:00:16\n",
      "   - -------------------------------------- 5.0/101.7 MB 6.4 MB/s eta 0:00:16\n",
      "   -- ------------------------------------- 6.3/101.7 MB 6.3 MB/s eta 0:00:16\n",
      "   -- ------------------------------------- 7.6/101.7 MB 6.4 MB/s eta 0:00:15\n",
      "   --- ------------------------------------ 9.2/101.7 MB 6.5 MB/s eta 0:00:15\n",
      "   ---- ----------------------------------- 10.5/101.7 MB 6.5 MB/s eta 0:00:14\n",
      "   ---- ----------------------------------- 11.8/101.7 MB 6.5 MB/s eta 0:00:14\n",
      "   ----- ---------------------------------- 13.1/101.7 MB 6.5 MB/s eta 0:00:14\n",
      "   ----- ---------------------------------- 14.4/101.7 MB 6.6 MB/s eta 0:00:14\n",
      "   ------ --------------------------------- 16.0/101.7 MB 6.6 MB/s eta 0:00:14\n",
      "   ------ --------------------------------- 17.3/101.7 MB 6.6 MB/s eta 0:00:13\n",
      "   ------- -------------------------------- 18.9/101.7 MB 6.7 MB/s eta 0:00:13\n",
      "   ------- -------------------------------- 20.2/101.7 MB 6.6 MB/s eta 0:00:13\n",
      "   -------- ------------------------------- 21.8/101.7 MB 6.7 MB/s eta 0:00:12\n",
      "   --------- ------------------------------ 23.1/101.7 MB 6.7 MB/s eta 0:00:12\n",
      "   --------- ------------------------------ 24.6/101.7 MB 6.7 MB/s eta 0:00:12\n",
      "   ---------- ----------------------------- 26.0/101.7 MB 6.7 MB/s eta 0:00:12\n",
      "   ---------- ----------------------------- 27.5/101.7 MB 6.7 MB/s eta 0:00:12\n",
      "   ----------- ---------------------------- 28.8/101.7 MB 6.7 MB/s eta 0:00:11\n",
      "   ----------- ---------------------------- 30.1/101.7 MB 6.7 MB/s eta 0:00:11\n",
      "   ------------ --------------------------- 31.7/101.7 MB 6.7 MB/s eta 0:00:11\n",
      "   ------------ --------------------------- 33.0/101.7 MB 6.7 MB/s eta 0:00:11\n",
      "   ------------- -------------------------- 34.3/101.7 MB 6.8 MB/s eta 0:00:10\n",
      "   -------------- ------------------------- 35.9/101.7 MB 6.8 MB/s eta 0:00:10\n",
      "   -------------- ------------------------- 37.2/101.7 MB 6.8 MB/s eta 0:00:10\n",
      "   --------------- ------------------------ 38.8/101.7 MB 6.8 MB/s eta 0:00:10\n",
      "   --------------- ------------------------ 40.1/101.7 MB 6.8 MB/s eta 0:00:10\n",
      "   ---------------- ----------------------- 41.4/101.7 MB 6.8 MB/s eta 0:00:09\n",
      "   ---------------- ----------------------- 43.0/101.7 MB 6.8 MB/s eta 0:00:09\n",
      "   ----------------- ---------------------- 44.6/101.7 MB 6.8 MB/s eta 0:00:09\n",
      "   ------------------ --------------------- 45.9/101.7 MB 6.8 MB/s eta 0:00:09\n",
      "   ------------------ --------------------- 47.4/101.7 MB 6.8 MB/s eta 0:00:08\n",
      "   ------------------- -------------------- 48.8/101.7 MB 6.8 MB/s eta 0:00:08\n",
      "   ------------------- -------------------- 50.3/101.7 MB 6.8 MB/s eta 0:00:08\n",
      "   -------------------- ------------------- 51.9/101.7 MB 6.8 MB/s eta 0:00:08\n",
      "   -------------------- ------------------- 53.2/101.7 MB 6.8 MB/s eta 0:00:08\n",
      "   --------------------- ------------------ 54.8/101.7 MB 6.8 MB/s eta 0:00:07\n",
      "   ---------------------- ----------------- 56.1/101.7 MB 6.8 MB/s eta 0:00:07\n",
      "   ---------------------- ----------------- 57.7/101.7 MB 6.8 MB/s eta 0:00:07\n",
      "   ----------------------- ---------------- 59.0/101.7 MB 6.8 MB/s eta 0:00:07\n",
      "   ----------------------- ---------------- 60.6/101.7 MB 6.8 MB/s eta 0:00:07\n",
      "   ------------------------ --------------- 61.9/101.7 MB 6.8 MB/s eta 0:00:06\n",
      "   ------------------------ --------------- 63.4/101.7 MB 6.8 MB/s eta 0:00:06\n",
      "   ------------------------- -------------- 64.7/101.7 MB 6.8 MB/s eta 0:00:06\n",
      "   -------------------------- ------------- 66.3/101.7 MB 6.8 MB/s eta 0:00:06\n",
      "   -------------------------- ------------- 67.6/101.7 MB 6.8 MB/s eta 0:00:05\n",
      "   -------------------------- ------------- 68.2/101.7 MB 6.8 MB/s eta 0:00:05\n",
      "   --------------------------- ------------ 69.2/101.7 MB 6.7 MB/s eta 0:00:05\n",
      "   --------------------------- ------------ 70.3/101.7 MB 6.7 MB/s eta 0:00:05\n",
      "   --------------------------- ------------ 70.3/101.7 MB 6.7 MB/s eta 0:00:05\n",
      "   ---------------------------- ----------- 71.3/101.7 MB 6.5 MB/s eta 0:00:05\n",
      "   ---------------------------- ----------- 72.6/101.7 MB 6.5 MB/s eta 0:00:05\n",
      "   ----------------------------- ---------- 74.2/101.7 MB 6.5 MB/s eta 0:00:05\n",
      "   ----------------------------- ---------- 75.5/101.7 MB 6.5 MB/s eta 0:00:05\n",
      "   ------------------------------ --------- 77.1/101.7 MB 6.5 MB/s eta 0:00:04\n",
      "   ------------------------------ --------- 78.4/101.7 MB 6.5 MB/s eta 0:00:04\n",
      "   ------------------------------- -------- 80.0/101.7 MB 6.5 MB/s eta 0:00:04\n",
      "   ------------------------------- -------- 81.3/101.7 MB 6.5 MB/s eta 0:00:04\n",
      "   -------------------------------- ------- 82.6/101.7 MB 6.5 MB/s eta 0:00:03\n",
      "   --------------------------------- ------ 84.1/101.7 MB 6.5 MB/s eta 0:00:03\n",
      "   --------------------------------- ------ 85.5/101.7 MB 6.5 MB/s eta 0:00:03\n",
      "   ---------------------------------- ----- 87.0/101.7 MB 6.6 MB/s eta 0:00:03\n",
      "   ---------------------------------- ----- 88.3/101.7 MB 6.6 MB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 89.9/101.7 MB 6.6 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 91.2/101.7 MB 6.6 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 92.8/101.7 MB 6.6 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 94.1/101.7 MB 6.6 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 95.7/101.7 MB 6.6 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 97.3/101.7 MB 6.6 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 98.6/101.7 MB 6.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  100.1/101.7 MB 6.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  101.4/101.7 MB 6.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  101.7/101.7 MB 6.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 101.7/101.7 MB 6.5 MB/s eta 0:00:00\n",
      "Using cached numpy-1.26.4-cp312-cp312-win_amd64.whl (15.5 MB)\n",
      "Installing collected packages: numpy, catboost\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.1.2\n",
      "    Uninstalling numpy-2.1.2:\n",
      "      Successfully uninstalled numpy-2.1.2\n",
      "Successfully installed catboost-1.2.7 numpy-1.26.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\Aditya kumar Dubey\\anaconda3\\envs\\dubey\\Lib\\site-packages\\~umpy.libs'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\Aditya kumar Dubey\\anaconda3\\envs\\dubey\\Lib\\site-packages\\~umpy'.\n",
      "  You can safely remove it manually.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "langchain 0.3.0 requires tenacity!=8.4.0,<9.0.0,>=8.1.0, which is not installed.\n",
      "langchain-community 0.3.0 requires tenacity!=8.4.0,<9.0.0,>=8.1.0, which is not installed.\n"
     ]
    }
   ],
   "source": [
    "! pip install catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d94dd955-1c0a-44c4-ab95-6b21401138f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "y = iris.target  # Target variable (0, 1, or 2 for the 3 classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0819f317-065f-42a7-9a35-af7f1aedf033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8bec7e8-0c58-4444-b930-f053bcc6a6d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostClassifier at 0x23d1711a510>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 3: Initialize and train the CatBoostClassifier\n",
    "model = CatBoostClassifier(iterations=100,  # Number of boosting rounds\n",
    "                           learning_rate=0.1,  # Step size\n",
    "                           depth=6,  # Tree depth\n",
    "                           loss_function='MultiClass',  # Since we have multiple classes\n",
    "                           verbose=0)  # Suppress output during training\n",
    "\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66adebc2-7045-40be-8035-8e5958c0302c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Make predictions\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "acef81ea-b42c-4cbc-9523-bd6a1e2b2e3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Model Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9c5e4b-f208-4710-a23f-8c939defa751",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
